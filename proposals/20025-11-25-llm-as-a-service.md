---
layout: default
title: "ğŸ§  LLM som service"
summary: "En skalerbar LLM-as-a-Service lÃ¸sning der leverer solidariske flatrate-priser og potentiale for energieffektiv drift"
author: "Jan Maack Kjerbye"
date: "2025-11-28"
tags: [AI, LLM, Kubernetes, KServe, ModelMesh, GrÃ¸n IT]
status: "Udkast"
parent: "Proposals"
published: true
---

{% include header_metadata.html %}

Udkast  
{: .label .label-yellow }

## Baggrund

Styregruppen i os2ai har lÃ¦nge Ã¸nsket sig en lÃ¸sning for *â€œLLM as a Serviceâ€* med:
- **Flatrate betaling** 
- **Solidaritetsmodel**


# Arkitektur anbefaling

> ### Det anbefales at anvende **KServe** med **ModelMesh** for at udnytte de muligheder vi allerede betaler for i vore k8s cluster, til at levere pÃ¥ de ovenstÃ¥ende Ã¸nsker.

Denne lÃ¸sning:
- Udnytter de k8s native **operators og funktionalitet** vi allerede betaler for.
- Genbruger eksisterende internationalt vedligeholdte lÃ¸sninger, istedet for at opfinde nye dybe tallerkener.
- UnderstÃ¸tter **model-sharing** for effektiv ressourceudnyttelse
- MuliggÃ¸r **scale-to-zero** og GPU-pooling for lavere energiforbrug og dermed forventet lavere hostingpriser
- Er **CNCF open source** og cloud-neutral

## Komponenter
_Arkitekturlandskab_

---

```mermaid
flowchart LR
    subgraph Application cluster
        A[BrugergrÃ¦nseflade]
        B[Backend]
    end
    subgraph GPU Cluster
        C[KServe Operator]
        D[vLLM Embedded i ModelMesh]
    end

    A --> B --> C --> D

    A:::Aqua
    B:::Aqua
    C:::Aqua
    D:::Aqua
    classDef Sky stroke-width:1px, stroke-dasharray:none, stroke:#374D7C, fill:#E2EBFF, color:#374D7C
    classDef Ash stroke-width:1px, stroke-dasharray:none, stroke:#999999, fill:#EEEEEE, color:#000000
    classDef Aqua stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
```

### [Kserve](https://kserve.github.io/website/)
> Skalerbar inferencing med multi-tenancy og dynamisk model-loading og skalering.

### [ModelMesh](https://github.com/kserve/modelmesh)
> Avanceret runtime til multi-model hosting med memory-optimering.
GÃ¸r det muligt at have mange modeller tilgÃ¦ngelige uden at alle fylder i GPU-hukommelsen samtidig.
Integrerer med KServe for dynamisk model-loading og routing.


# Forventede gevinster

### ğŸ’° Fair og forudsigelig Ã¸konomi
> Faste tiers med flatrate muliggÃ¸r solidarisk prisstruktur.

### ğŸŒ± GrÃ¸n IT og lavere hostingpris
> Scale-to-zero og GPU-pooling reducerer energiforbrug og driftsomkostninger markant.

### ğŸ”’ Robust og fremtidssikret
> CNCF open source og Kubernetes-native operators sikrer standardisering og leverandÃ¸ruafhÃ¦ngighed.

## Anvendte arkitekturprincipper  
Forslaget understÃ¸tter fÃ¸lgende fÃ¦llesoffentlige principper og regler:

[â™»ï¸ Genbrug og fÃ¦lles lÃ¸sninger](https://arkitektur.digst.dk/principper-og-regler){: .btn .btn-green } [ğŸ‘ï¸ Ã…bne standarder og interoperabilitet](https://arkitektur.digst.dk/principper-og-regler){: .btn .btn-green } [ğŸ§© Modularitet og lÃ¸skobling](https://arkitektur.digst.dk/principper-og-regler){: .btn .btn-green } [ğŸ”’ Sikkerhed og robusthed](https://arkitektur.digst.dk/principper-og-regler){: .btn .btn-green }  [ğŸŒ± GrÃ¸n IT og effektiv ressourceudnyttelse](https://arkitektur.digst.dk/principper-og-regler){: .btn .btn-green } [ğŸ“ Standardisering og governance](https://arkitektur.digst.dk/principper-og-regler){: .btn .btn-green }  

## Kilder

- https://developer.ibm.com/articles/llms-inference-scaling-vllm-kserve/
- https://developer.ibm.com/blogs/kserve-and-watson-modelmesh-extreme-scale-model-inferencing-for-trusted-ai/